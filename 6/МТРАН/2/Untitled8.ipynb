{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVKu8rCkC1C5"
      },
      "source": [
        "def tokenize(token, category, tokens):\n",
        "  if len(token) == 0 or category == \"whitespace\" or \"\\n\" in token or \" \" in token:\n",
        "    pass\n",
        "  elif category == \"letter\":\n",
        "    tokenize_letter(token, tokens)\n",
        "  else:\n",
        "    tokens.append({\"token\": token, \"category\": category, \"row\": get_row(), \"column\": get_column()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "furXcDra8RtX"
      },
      "source": [
        "def get_row():\n",
        "  global row\n",
        "  return row\n",
        "\n",
        "def get_column():\n",
        "  global col\n",
        "  return col + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmVr2xHNGJSI"
      },
      "source": [
        "def tokenize_letter(token, tokens):\n",
        "  RESERVED_LIST = ['using', 'namespace', 'int', 'main', 'if', 'else', 'cin', 'cout', 'return', 'for', 'struct']\n",
        "  if token in RESERVED_LIST:\n",
        "    tokens.append({\"token\": token, \"category\": \"Reserved word\", \"row\": get_row(), \"column\": get_column()})\n",
        "  else:\n",
        "    tokens.append({\"token\": token, \"category\": \"Variable or namespace\", \"row\": get_row(), \"column\": get_column()})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn8-2Eop_AOd"
      },
      "source": [
        "def tokenize_directiva(file, tokens):\n",
        "  global col\n",
        "  \n",
        "  if col == 1:\n",
        "    token = '#'\n",
        "    c = 'q'\n",
        "    while c != '\\n':\n",
        "      c = read_symbol(file)\n",
        "      token += c\n",
        "    token = token[:-1]\n",
        "    tokens.append({\"token\": token, \"category\": \"include\", \"row\": get_row() - 1, \"column\": get_column()})\n",
        "  \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwJPMjEqKo4k"
      },
      "source": [
        "def tokenize_string(file, tokens):\n",
        "  token = ''\n",
        "  c = 'q'\n",
        "  while c != '\"':\n",
        "    c = read_symbol(file)\n",
        "    token += c\n",
        "  token = token[:-1]\n",
        "  tokens.append({\"token\":token, \"category\": \"String literal\", \"row\": get_row(), \"column\": get_column()})\n",
        "  tokens.append({\"token\":'\"', \"category\": \"double quanties\", \"row\": get_row(), \"column\": get_column()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it6gmnskLq7U"
      },
      "source": [
        "def tokenize_char(file, tokens):\n",
        "  token = ''\n",
        "  c = 'q'\n",
        "  while c != '\"':\n",
        "    c = read_symbol(file)\n",
        "    token += c\n",
        "  token = token[:-1]\n",
        "  tokens.append({\"token\":token, \"category\": \"char literal\", \"row\": get_row(), \"column\": get_column()})\n",
        "  tokens.append({\"token\":'\"', \"category\": \"single quanties\", \"row\": get_row(), \"column\": get_column()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5BJ4Q9w-kNI"
      },
      "source": [
        "def get_category(c):\n",
        "  if c.isnumeric():\n",
        "    return \"number\"\n",
        "  if c == '#':\n",
        "    return \"Directiva\"\n",
        "  if c.isalpha():\n",
        "    return \"letter\"\n",
        "  if c == ' ':\n",
        "    return \"whitespace\"\n",
        "  if c == ';':\n",
        "    return \"operator split\"\n",
        "  if c == ',':\n",
        "    return 'comma'\n",
        "  if c == '.':\n",
        "    return 'dot'\n",
        "  if c in \"([{\":\n",
        "    return \"open bracket\"\n",
        "  if c in \")]}\":\n",
        "    return \"close bracket\"\n",
        "  if c in \"+/=<>-%\":\n",
        "    return \"operators\"\n",
        "  if c == '\"':\n",
        "    return 'double quanties'\n",
        "  if c == \"'\":\n",
        "    return 'single quanties'\n",
        "  return 'Unknown'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgHQsaN98lY_"
      },
      "source": [
        "def read_symbol(file):\n",
        "  global pos, row, col\n",
        "  c = file.read(1)\n",
        "  \n",
        "  if  c == '\\n':\n",
        "    row += 1\n",
        "    col = 0\n",
        "  elif c:\n",
        "    col += 1\n",
        "  else:\n",
        "    return 0\n",
        "  return c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NtPcArmx5Jy"
      },
      "source": [
        "operators = ['=', '<', '>', '+', '-', '<<', '==', '--', '++', '/']\n",
        "def errorizer(lines, tokens):\n",
        "  print(\"ERRORS\")\n",
        "  for i in range(1, len(tokens)):\n",
        "    if tokens[i]['category'] == 'operators' and tokens[i]['token'] not in operators:\n",
        "      print(f\"line {tokens[i]['row']} char {tokens[i]['column']} :: Неизвестный оператор\")\n",
        "      print(lines[tokens[i]['row'] - 1][:-1])\n",
        "      print(' ' *  tokens[i]['column'] + '^')\n",
        "    elif tokens[i]['category'] == tokens[i - 1]['category'] and tokens[i]['category'] != 'Reserved word':\n",
        "      print(f\"line {tokens[i - 1]['row']} char {tokens[i - 1]['column']} :: expected initializer before '{tokens[i - 1]['token']}'\")\n",
        "      print(lines[tokens[i]['row'] - 1][:-1])\n",
        "      print(' ' * tokens[i - 1]['column'] + '^')\n",
        "    \n",
        "  for i in range(len(lines)):\n",
        "    if len(lines[i]) == 1:\n",
        "      continue\n",
        "    if lines[i][-2] == ';' and (lines[i][-3] == '{' or lines[i][-3] == '}' or lines[i][-3] == '>'):\n",
        "      print(f\"line {i + 1} char {len(lines[i])} :: Unxpected ';'\")\n",
        "      print(lines[tokens[i]['row']][:-1])\n",
        "      print(' ' * (len(lines[i]) - 2) + '^')\n",
        "    if lines[i][-2] != ';' and (lines[i][-3] != '{' and lines[i][-3] != '}' and lines[i][-3] != '>'):\n",
        "      print(f\"line {i + 1} char {len(lines[i])} :: Expected ';'\")\n",
        "      print(lines[tokens[i]['row']][:-1])\n",
        "      print(' ' * (len(lines[i]) - 2) + '^')\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z33HZczY8Iq3",
        "outputId": "5325e919-c472-4f6c-e101-f1f18e53d3c8"
      },
      "source": [
        "col, row = 0, 1\n",
        "file = open(\"main.cpp\")\n",
        "f = open(\"main.cpp\")\n",
        "Lines = f.readlines()\n",
        "tokens = []\n",
        "pattern = \"\"\n",
        "pred_category, category = \"\", \"\"\n",
        "while True:\n",
        "  c = read_symbol(file)\n",
        "  if c == 0:\n",
        "    break\n",
        "  pred_category = category\n",
        "  category = get_category(c)\n",
        "  if category == \"Directiva\":\n",
        "    tokenize_directiva(file, tokens)\n",
        "    continue\n",
        "  elif category == \"double quanties\":\n",
        "    tokenize_string(file, tokens)\n",
        "    continue\n",
        "  elif category == \"single quanties\":\n",
        "    tokenize_char(file, tokens)\n",
        "    continue\n",
        "  elif category == \"\":\n",
        "    tokenize_string(file, tokens)\n",
        "  \n",
        "  elif pred_category != category:\n",
        "    pass\n",
        "    tokenize(pattern, pred_category, tokens)\n",
        "    pattern = \"\"\n",
        "  pattern += c\n",
        "errorizer(Lines, tokens)    \n",
        "categories = []\n",
        "for token in tokens:\n",
        "  categories.append(token['category'])\n",
        "categories = set(categories)\n",
        "print('TOKENS')\n",
        "for category in categories:\n",
        "  print(\"_____________\")\n",
        "  print(f\"CATEGORY: {category}\")\n",
        "  for token in tokens:\n",
        "    if token['category'] == category:\n",
        "      print(f\"TOKEN: {token['token']} POSITION: ({token['row']}, {token['column']})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "ERRORS\n",
            "line 1 char 19 :: Expected ';'\n",
            "using namespace std;\n",
            "                 ^\n",
            "line 3 char 13 :: Unxpected ';'\n",
            "int main(){;\n",
            "           ^\n",
            "line 10 char 18 :: Expected ';'\n",
            "  int a[10] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n",
            "                ^\n",
            "line 16 char 4 :: Expected ';'\n",
            "  int n = 10, key = 5;\n",
            "  ^\n",
            "line 20 char 60 :: Expected ';'\n",
            "  int n = 10, key = 5;\n",
            "                                                          ^\n",
            "TOKENS\n",
            "_____________\n",
            "CATEGORY: operators\n",
            "TOKEN: = POSITION: (4, 15)\n",
            "TOKEN: = POSITION: (5, 11)\n",
            "TOKEN: = POSITION: (5, 21)\n",
            "TOKEN: = POSITION: (6, 11)\n",
            "TOKEN: = POSITION: (7, 11)\n",
            "TOKEN: < POSITION: (10, 14)\n",
            "TOKEN: = POSITION: (11, 11)\n",
            "TOKEN: + POSITION: (11, 16)\n",
            "TOKEN: / POSITION: (11, 21)\n",
            "TOKEN: > POSITION: (13, 18)\n",
            "TOKEN: = POSITION: (13, 27)\n",
            "TOKEN: = POSITION: (14, 14)\n",
            "TOKEN: + POSITION: (14, 20)\n",
            "TOKEN: -- POSITION: (17, 7)\n",
            "TOKEN: == POSITION: (19, 15)\n",
            "TOKEN: << POSITION: (19, 28)\n",
            "TOKEN: << POSITION: (19, 50)\n",
            "TOKEN: << POSITION: (19, 57)\n",
            "TOKEN: << POSITION: (19, 81)\n",
            "TOKEN: << POSITION: (20, 16)\n",
            "_____________\n",
            "CATEGORY: Reserved word\n",
            "TOKEN: using POSITION: (2, 7)\n",
            "TOKEN: namespace POSITION: (2, 17)\n",
            "TOKEN: int POSITION: (3, 5)\n",
            "TOKEN: main POSITION: (3, 10)\n",
            "TOKEN: int POSITION: (4, 7)\n",
            "TOKEN: int POSITION: (5, 7)\n",
            "TOKEN: int POSITION: (6, 7)\n",
            "TOKEN: int POSITION: (7, 7)\n",
            "TOKEN: int POSITION: (8, 7)\n",
            "TOKEN: if POSITION: (13, 8)\n",
            "TOKEN: else POSITION: (14, 10)\n",
            "TOKEN: if POSITION: (19, 6)\n",
            "TOKEN: cout POSITION: (19, 25)\n",
            "TOKEN: else POSITION: (20, 8)\n",
            "TOKEN: cout POSITION: (20, 13)\n",
            "_____________\n",
            "CATEGORY: double quanties\n",
            "TOKEN: \" POSITION: (19, 46)\n",
            "TOKEN: \" POSITION: (19, 77)\n",
            "TOKEN: \" POSITION: (20, 60)\n",
            "_____________\n",
            "CATEGORY: operator split\n",
            "TOKEN: ; POSITION: (3, 1)\n",
            "TOKEN: ; POSITION: (4, 1)\n",
            "TOKEN: ; POSITION: (5, 1)\n",
            "TOKEN: ; POSITION: (6, 1)\n",
            "TOKEN: ; POSITION: (7, 1)\n",
            "TOKEN: ; POSITION: (8, 1)\n",
            "TOKEN: ; POSITION: (9, 1)\n",
            "TOKEN: ; POSITION: (12, 1)\n",
            "TOKEN: ; POSITION: (14, 1)\n",
            "TOKEN: ; POSITION: (15, 1)\n",
            "TOKEN: ; POSITION: (18, 1)\n",
            "TOKEN: ; POSITION: (20, 1)\n",
            "_____________\n",
            "CATEGORY: comma\n",
            "TOKEN: , POSITION: (4, 19)\n",
            "TOKEN: , POSITION: (4, 22)\n",
            "TOKEN: , POSITION: (4, 25)\n",
            "TOKEN: , POSITION: (4, 28)\n",
            "TOKEN: , POSITION: (4, 31)\n",
            "TOKEN: , POSITION: (4, 34)\n",
            "TOKEN: , POSITION: (4, 37)\n",
            "TOKEN: , POSITION: (4, 40)\n",
            "TOKEN: , POSITION: (4, 43)\n",
            "TOKEN: , POSITION: (5, 15)\n",
            "_____________\n",
            "CATEGORY: number\n",
            "TOKEN: 10 POSITION: (4, 12)\n",
            "TOKEN: 1 POSITION: (4, 18)\n",
            "TOKEN: 2 POSITION: (4, 21)\n",
            "TOKEN: 3 POSITION: (4, 24)\n",
            "TOKEN: 4 POSITION: (4, 27)\n",
            "TOKEN: 5 POSITION: (4, 30)\n",
            "TOKEN: 6 POSITION: (4, 33)\n",
            "TOKEN: 7 POSITION: (4, 36)\n",
            "TOKEN: 8 POSITION: (4, 39)\n",
            "TOKEN: 9 POSITION: (4, 42)\n",
            "TOKEN: 10 POSITION: (4, 46)\n",
            "TOKEN: 10 POSITION: (5, 14)\n",
            "TOKEN: 5 POSITION: (5, 23)\n",
            "TOKEN: 0 POSITION: (6, 13)\n",
            "TOKEN: 2 POSITION: (11, 23)\n",
            "TOKEN: 1 POSITION: (14, 22)\n",
            "_____________\n",
            "CATEGORY: String literal\n",
            "TOKEN: Индекс элемента  POSITION: (19, 46)\n",
            "TOKEN:  в массиве равен:  POSITION: (19, 77)\n",
            "TOKEN: Извините, но такого элемента в массиве нет POSITION: (20, 60)\n",
            "_____________\n",
            "CATEGORY: Variable or namespace\n",
            "TOKEN: std POSITION: (2, 21)\n",
            "TOKEN: a POSITION: (4, 9)\n",
            "TOKEN: n POSITION: (5, 9)\n",
            "TOKEN: key POSITION: (5, 19)\n",
            "TOKEN: l POSITION: (6, 9)\n",
            "TOKEN: r POSITION: (7, 9)\n",
            "TOKEN: n POSITION: (7, 13)\n",
            "TOKEN: mid POSITION: (8, 11)\n",
            "TOKEN: while POSITION: (10, 9)\n",
            "TOKEN: l POSITION: (10, 12)\n",
            "TOKEN: r POSITION: (10, 16)\n",
            "TOKEN: mid POSITION: (11, 9)\n",
            "TOKEN: l POSITION: (11, 14)\n",
            "TOKEN: r POSITION: (11, 18)\n",
            "TOKEN: a POSITION: (13, 11)\n",
            "TOKEN: mid POSITION: (13, 15)\n",
            "TOKEN: key POSITION: (13, 22)\n",
            "TOKEN: r POSITION: (13, 25)\n",
            "TOKEN: mid POSITION: (13, 31)\n",
            "TOKEN: l POSITION: (14, 12)\n",
            "TOKEN: mid POSITION: (14, 18)\n",
            "TOKEN: r POSITION: (17, 5)\n",
            "TOKEN: a POSITION: (19, 9)\n",
            "TOKEN: r POSITION: (19, 11)\n",
            "TOKEN: key POSITION: (19, 19)\n",
            "TOKEN: key POSITION: (19, 54)\n",
            "TOKEN: r POSITION: (19, 83)\n",
            "_____________\n",
            "CATEGORY: close bracket\n",
            "TOKEN: ) POSITION: (3, 12)\n",
            "TOKEN: ] POSITION: (4, 13)\n",
            "TOKEN: ] POSITION: (4, 47)\n",
            "TOKEN: ) POSITION: (10, 17)\n",
            "TOKEN: ) POSITION: (11, 19)\n",
            "TOKEN: ] POSITION: (13, 16)\n",
            "TOKEN: ) POSITION: (13, 23)\n",
            "TOKEN: } POSITION: (17, 1)\n",
            "TOKEN: ] POSITION: (19, 12)\n",
            "TOKEN: ) POSITION: (19, 20)\n",
            "_____________\n",
            "CATEGORY: include\n",
            "TOKEN: #include<iostream> POSITION: (1, 1)\n",
            "_____________\n",
            "CATEGORY: open bracket\n",
            "TOKEN: ( POSITION: (3, 11)\n",
            "TOKEN: { POSITION: (3, 13)\n",
            "TOKEN: [ POSITION: (4, 10)\n",
            "TOKEN: [ POSITION: (4, 17)\n",
            "TOKEN: ( POSITION: (10, 11)\n",
            "TOKEN: { POSITION: (11, 1)\n",
            "TOKEN: ( POSITION: (11, 13)\n",
            "TOKEN: ( POSITION: (13, 10)\n",
            "TOKEN: [ POSITION: (13, 12)\n",
            "TOKEN: ( POSITION: (19, 8)\n",
            "TOKEN: [ POSITION: (19, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TBr4qsLwDas"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFXXUgXZ89Ig"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}